{"cells":[{"cell_type":"markdown","metadata":{},"source":["# TP2 INFO8003\n","The idea behind this notebook is to get familiar with the state-action value function $Q(s,a)$ and learning this function using model-based and model-free approaches for small state-action spaces."]},{"cell_type":"markdown","metadata":{},"source":["## Environment description \n","\n","Jack is searching for treasures on an island represented by a 5x5 grid, where each cell corresponds to a state. Jack starts at the bottom-left corner, and his goal is to navigate the grid to collect treasures. There are two types of treasures: a small one at (2, 2) that rewards him with 1 gold each time he collects it, and a large one at (4, 4) that rewards him with 10 gold each time. Walls are present at (1, 1), (2, 1), and (2, 3), and Jack cannot pass through these cells (they are not considered as state therefore).\n","\n","Beware of cliffs at (4, 1), (4, 2), and (4, 3), as falling off a cliff will cost Jack 100 gold. Additionally, some cells are slippery (the blue ones shown in the figure below), increasing the likelihood of falling into a cliff.\n","\n","Since Jack has consumed some rum, he has an 80% chance of performing the intended action and a 20% chance of performing a random action. He can move right, left, up, or down. The environment will display Jackâ€™s position and the layout of the grid.\n","\n","<p align=\"center\"> \n","    <img src=\"GridEnv.png\" alt=\"GridEnv visualization\">\n","    </p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym \n","from gymnasium import spaces\n","import numpy as np\n","\n","class GridEnv(gym.Env):\n","    def __init__(self):\n","        super(GridEnv, self).__init__()\n","        self.grid_size = 5\n","        self.start_state = (4, 0)\n","        self.small_treasure_state = (2, 2)\n","        self.big_treasure_state = (4, 4)\n","        self.wall_states = [(1, 1), (2, 1), (2, 3)]\n","        self.cliff_states = [(4, 1), (4, 2), (4, 3)]\n","        self.slippery_states = [(3, 1), (3, 2), (3, 3)]\n","        \n","        # Actions: 0=Right, 1=Left, 2=Up, 3=Down\n","        self.action_space = spaces.Discrete(4)\n","        self.observation_space = spaces.Discrete(self.grid_size * self.grid_size-len(self.wall_states))\n","        \n","        self.state = self.encode_state(self.start_state)\n","\n","    def encode_state(self, state_tuple):\n","        x, y = state_tuple\n","        return x  * self.grid_size + y \n","\n","    def decode_state(self, state_int):\n","        x = state_int // self.grid_size\n","        y = state_int % self.grid_size\n","        return (x, y)\n","\n","    def step(self, action):\n","        x, y = self.decode_state(self.state)\n","        new_x, new_y = x, y\n","\n","        # Check if Jack takes an action at random or not\n","        w = np.random.uniform()\n","        if w < 0.2:\n","            action = self.action_space.sample()\n","\n","        if action == 0:  # right\n","            new_y = min(y + 1, self.grid_size - 1)\n","        elif action == 1:  # left\n","            new_y = max(y - 1, 0)\n","        elif action == 2:  # up\n","            new_x = max(x - 1, 0)\n","        elif action == 3:  # down\n","            new_x = min(x + 1, self.grid_size - 1)\n","\n","        # Check if the new position is a wall\n","        if (new_x, new_y) in self.wall_states:\n","            self.state = self.encode_state((x, y))\n","        else:\n","            self.state = self.encode_state((new_x, new_y))\n","        \n","        if (new_x, new_y) == self.big_treasure_state:\n","            reward = 10\n","        elif (new_x, new_y) in self.cliff_states:\n","            reward = -100\n","        elif (new_x, new_y) == self.small_treasure_state:\n","            reward = 1\n","        else:\n","            reward = 0\n","        \n","        done = False\n","        truncated = False\n","        \n","        return self.state, reward, done, truncated, {}\n","\n","    def reset(self):\n","        self.state = self.encode_state(self.start_state)\n","        return self.state, {}\n","\n","    def render(self, mode='human'):\n","        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n","        grid[:] = '.'\n","        grid[self.big_treasure_state] = 'T'\n","        grid[self.small_treasure_state] = 't'\n","        for cliff in self.cliff_states:\n","            grid[cliff] = 'C'\n","        for wall in self.wall_states:\n","            grid[wall] = 'W'\n","        x, y = self.decode_state(self.state)\n","        grid[x, y] = 'J'\n","        print(\"\\n\".join([\"\".join(row) for row in grid]))\n","        print()\n","\n","env = GridEnv()\n","state = env.reset()\n","env.render()\n","\n","for _ in range(10):\n","    action = env.action_space.sample()  # Random action\n","    state, reward, done, truncated, _ = env.step(action)\n","    print(f\"Action: {action}, Reward: {reward}\")\n","    env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","env = GridEnv()\n","\n","def simulate_policy(env, policy, steps=10):\n","    \"\"\"\n","    Simulate a policy in the given environment over a specified number of steps.\n","\n","    Parameters:\n","    env (Env): The environment in which the policy is to be simulated.\n","    policy (function): A function that takes a state as input and returns an action.\n","    steps (int): The number of steps to simulate the policy for.\n","\n","    Returns:\n","    list: A list representing the trajectory, i.e. (s_0, a_0, r_0, s_1, a_1, r_1,..., a_{t-1}, r_{t-1}, s_t).\n","    \"\"\"\n","    state, _ = env.reset()\n","    trajectory = [state]\n","    for _ in range(steps):\n","        action = policy(state)\n","        next_state, reward, _, _, _ = env.step(action)\n","        trajectory.append(action)\n","        trajectory.append(reward)\n","        trajectory.append(next_state)\n","        state = next_state\n","\n","    return trajectory\n","\n","def random_policy(state):\n","    \"\"\"\n","    Policy taking an action at random in [0, 4[.\n","\n","    Parameters:\n","    state (int): the current state.\n","\n","    Returns:\n","    action (int): a random action.\n","    \"\"\"\n","    action = np.random.randint(4)\n","    return action\n","\n","trajectory =  simulate_policy(env, random_policy, 100000)\n","print(\"trajectory:\", trajectory)"]},{"cell_type":"markdown","metadata":{},"source":["## Question 1\n","Implement a routine which estimates $r(s,a)$ and $p(s'|s,a)$ from a given trajectory $h_t = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, a_{t-1}, r_{t-1}, s_t)$ produced by a random policy. Justify why your routine converges towards the true values for a growing trajectory ? \n","\n","Advice: take a trajectory large enough to estimate as close as possible the true mdp structure."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","def estimate_mdp_from_trajectory(trajectory):\n","    \"\"\"\n","    Estimate the reward function r(s, a) and transition probabilities p(s'|s, a) from a given trajectory.\n","    Hint: use defaultdict structure.\n","\n","    Parameters:\n","    trajectory (list): A list representing the trajectory, i.e. (s_0, a_0, r_0, s_1, a_1, r_1,..., a_{t-1}, r_{t-1}, s_t).\n","    \n","    Returns:\n","    estimated_rewards (dict): Estimated reward function with the structure {(s, a): mean_reward, ...}\n","    estimated_transitions (dict): Estimated transition probabilities with the strcuture {(s, a): {s_prime: probability, ...}, ...}.\n","    \"\"\"\n","    \n","\n","    # Implementation goes here\n","\n","    estimated_rewards = dict(sorted(estimated_rewards.items()))\n","    estimated_transitions = dict(sorted(estimated_transitions.items()))\n","\n","    return estimated_rewards, estimated_transitions\n","\n","steps = #...\n","estimate_mdp_from_trajectory(simulate_policy(env, random_policy, steps))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Question 2\n","Compute $\\widehat{Q}$ by using $\\widehat{r}(s,a)$ and $\\widehat{p}(s'|s,a)$ that estimate the MDP structure computed in question 1.  Explain the influence of the length of the trajectory on the quality of the approximation $\\hat{Q}$. Derive $\\widehat{\\mu}^*$ from $\\widehat{Q}$.\n","\n","Display $V_N^{\\widehat{\\mu}^*}$ for each state $s$. Supposing you estimate the true $Q_N$ functions, determine the smallest N such that the bound $V^{\\mu^*}-V^{\\mu^*_N}$ is smaller than 0.1 ? Justify."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def estimated_q_n(estimated_rewards, estimated_transitions, gamma=0.95, N=500):\n","    \"\"\"\n","    Compute the Q-values using the estimated rewards and transition probabilities.\n","\n","    Parameters:\n","    estimated_rewards (dict): Estimated reward function.\n","    estimated_transitions (dict): Estimated transition probabilities.\n","    gamma (float): Discount factor.\n","    iterations (int): Number of iterations for convergence.\n","\n","    Returns:\n","    dict: Estimated Q_N-values with the structure  {(s, a): value, ...}.\n","    \"\"\"\n","    estimated_q_n = defaultdict(lambda: 0)\n","\n","    # Implementation goes here\n","\n","    return dict(estimated_q_n)\n","\n","def derive_optimal_policy(q_values):\n","    \"\"\"\n","    Derive the optimal policy from Q-values.\n","\n","    Parameters:\n","    q_values (dict): Q-values.\n","\n","    Returns:\n","    dict: Optimal policy mapping states to actions with the structure  {s: a, ...}.\n","    \"\"\"\n","    policy = {}\n","    \n","    # Implementation goes here\n","\n","    return policy\n","\n","\n","def compute_value_function(q_values, policy):\n","    \"\"\"\n","    Compute the value function from Q-values and a policy.\n","\n","    Parameters:\n","    q_values (dict): Q-values.\n","    policy (dict): Policy mapping states to actions.\n","\n","    Returns:\n","    dict: Value function for each state with the structure  {s: value, ...}.\n","    \"\"\"\n","    value_function = {}\n","    \n","    # Implementation goes here\n","\n","    return value_function\n","\n","estimated_rewards, estimated_transitions = estimate_mdp_from_trajectory(simulate_policy(env, random_policy, 1000000))\n","q_values = estimated_q_n(estimated_rewards, estimated_transitions)\n","\n","optimal_policy = derive_optimal_policy(q_values)\n","value_function = compute_value_function(q_values, optimal_policy)\n","\n","print(\"Estimated Q-values:\", q_values)\n","print(\"Optimal Policy:\", optimal_policy)\n","print(\"Value Function:\", value_function)\n","print(\"Bound:\", )\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Display your optimal policy using the following function. Describe your result, is it coherent ?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display, Math\n","\n","def display_policy_as_latex_table(policy, grid_size):\n","    \"\"\"\n","    Display the optimal policy as a LaTeX table with arrows.\n","\n","    Parameters:\n","    policy (dict): Optimal policy mapping states to actions.\n","    grid_size (int): The size of the grid (assuming a square grid).\n","\n","    Returns:\n","    None: Displays the LaTeX table in the notebook.\n","    \"\"\"\n","    action_symbols = {\n","        0: r'\\rightarrow',\n","        1: r'\\leftarrow',\n","        2: r'\\uparrow',\n","        3: r'\\downarrow'\n","    }\n","\n","    latex_table = \"\\\\begin{array}{\" + \"c\" * grid_size + \"}\\n\"\n","\n","    for i in range(grid_size):\n","        row = []\n","        for j in range(grid_size):\n","            state = i * grid_size + j\n","            action = policy.get(state, None)\n","            if action is not None:\n","                row.append(action_symbols[action])\n","            else:\n","                row.append(\"\")\n","\n","        latex_table += \" & \".join(row) + \" \\\\\\\\\\n\"\n","\n","    latex_table += \"\\\\end{array}\"\n","\n","    display(Math(latex_table))\n","\n","display_policy_as_latex_table(optimal_policy, grid_size=5)"]},{"cell_type":"markdown","metadata":{},"source":["## Question 3\n","\n","Sample a trajectory from a random uniform policy with a finite, large enough time horizon $T$. Motivate your choice of $T$. Implement a routine which iteratively updates $\\widehat{Q}(s_k,a_k)$ using the Q-learning algorithm from this trajectory. Run your routine with a constant learning rate $\\alpha = 0.05$. Derive directly $\\widehat{\\mu}^*$ from $\\widehat{Q}$ and display this optimal policy. Display $V_N^{\\widehat{\\mu}^*}$ for each initial state $s$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def q_learning_update(trajectory, alpha=0.05, gamma=0.95):\n","    \"\"\"\n","    Compute the Q-values using the Q-learning algorithm.\n","\n","    Parameters:\n","    estimated_rewards (dict): Estimated reward function.\n","    estimated_transitions (dict): Estimated transition probabilities.\n","    gamma (float): Discount factor.\n","    iterations (int): Number of iterations for convergence.\n","\n","    Returns:\n","    dict: Estimated Q-values with the structure  {(s, a): value, ...}.\n","    \"\"\"\n","    q_values = defaultdict(lambda: 0)\n","\n","    # Implementation goes here\n","    \n","    return q_values\n","\n","\n","env = GridEnv()\n","T = #...\n","trajectory = simulate_policy(env, random_policy, T)\n","q_values = q_learning_update(trajectory)\n","\n","optimal_policy = derive_optimal_policy(q_values)\n","\n","display_policy_as_latex_table(optimal_policy, grid_size=5)\n","\n","value_function = compute_value_function(q_values, optimal_policy)\n","print(\"value function\", value_function)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Question 4\n","\n","Implement an intelligent agent which learns $Q$ with Q-learning using an $\\epsilon$-greedy policy, i.e. a policy that is greedy with a probability of $(1-\\epsilon)$ and random otherwise.\n","\n","Run the three following experimental protocols and display for each of them the derived optimal policy and their value function. Discuss the results.\n","\n","The first experimental protocol is the following. The agent trains over $100$ episodes having $1000$ transitions. An episode always starts from the initial state. The learning rate $\\alpha$ is equal to $0.05$ and the exploration rate $\\epsilon$ is equal to $0.5$. The values of $\\alpha$ and $\\epsilon$ are both constant over time. The function $\\hat{Q}$ is updated after every transition. The transitions are used only once for updating $\\hat{Q}$ i.e. there are not stored in a replay buffer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def epsilon_greedy_policy(q_values, state, epsilon):\n","    \"\"\"\n","    Selects an action using the epsilon-greedy policy.\n","\n","    Parameters:\n","    q_values (dict): A dictionary where keys are tuples of (state, action) and values are the estimated Q-values.\n","    state (tuple): The current state for which the action is to be selected.\n","    epsilon (float): The probability of selecting a random action (exploration). Should be between 0 and 1.\n","\n","    Returns:\n","    int: The action selected according to the epsilon-greedy policy. With probability epsilon, a random action is chosen.\n","         With probability 1-epsilon, the action with the highest Q-value for the given state is chosen.\n","    \"\"\"\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(4)\n","    else:\n","        return np.argmax(q_values[(state, a_prime)] for a_prime in range(4))\n","\n","def q_learning(env, episodes=100, alpha=0.05, epsilon=0.5, gamma=0.95):\n","    \"\"\"\n","    Perform Q-learning to learn the optimal Q-values with the first protocol.\n","\n","    Parameters:\n","    env (Env): The environment in which Q-learning is performed.\n","    episodes (int): The number of episodes to run Q-learning.\n","    alpha (float): The learning rate for updating Q-values.\n","    epsilon (float): The probability of selecting a random action (exploration) in epsilon-greedy policy.\n","    gamma (float): The discount factor for future rewards.\n","\n","    Returns:\n","    dict: A dictionary containing the learned Q-values for state-action pairs.\n","    \"\"\"\n","    q_values = defaultdict(lambda: 0)\n","    \n","    # Implementation goes here\n","\n","    return q_values\n","\n","env = GridEnv()\n","q_values = q_learning(env)\n","optimal_policy = derive_optimal_policy(q_values)\n","display_policy_as_latex_table(optimal_policy, grid_size=5)\n","\n","value_function = compute_value_function(q_values, optimal_policy)\n","print(\"value function\", value_function)\n"]},{"cell_type":"markdown","metadata":{},"source":["The second experimental protocol differs from the first one only by the learning rate which is such that $\\alpha_0 = 0.05$ and $\\forall t > 0, \\alpha_t = 0.8\\alpha_{t-1}$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def q_learning_2(env, episodes=100, alpha=0.05, epsilon=0.5, gamma=0.95):\n","    \"\"\"\n","    Perform Q-learning to learn the optimal Q-values with the second protocol.\n","\n","    Parameters:\n","    env (Env): The environment in which Q-learning is performed.\n","    episodes (int): The number of episodes to run Q-learning.\n","    alpha (float): The learning rate for updating Q-values.\n","    epsilon (float): The probability of selecting a random action (exploration) in epsilon-greedy policy.\n","    gamma (float): The discount factor for future rewards.\n","\n","    Returns:\n","    dict: A dictionary containing the learned Q-values for state-action pairs.\n","    \"\"\"\n","    q_values = defaultdict(lambda: 0)\n","    \n","    # Implementation goes here\n","    \n","    return q_values\n","\n","env = GridEnv()\n","q_values = q_learning(env)\n","optimal_policy = derive_optimal_policy(q_values)\n","display_policy_as_latex_table(optimal_policy, grid_size=5)\n","\n","value_function = compute_value_function(q_values, optimal_policy)\n","print(\"value function\", value_function)"]},{"cell_type":"markdown","metadata":{},"source":["In the following, a replay buffer is defined as a data structure of previously seen transitions i.e. $(s_t, a_t, r_t, s_{t+1})$. The third experimental protocol is identical from the first one except that the one-step system transitions are stored in a replay buffer and that at each time-step, the function $\\hat{Q}$ is updated ten times by drawing ten transitions at random from the replay buffer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def q_learning_3(env, episodes=100, alpha=0.05, epsilon=0.5, gamma=0.95):\n","    \"\"\"\n","    Perform Q-learning to learn the optimal Q-values with the third protocol.\n","\n","    Parameters:\n","    env (Env): The environment in which Q-learning is performed.\n","    episodes (int): The number of episodes to run Q-learning.\n","    alpha (float): The learning rate for updating Q-values.\n","    epsilon (float): The probability of selecting a random action (exploration) in epsilon-greedy policy.\n","    gamma (float): The discount factor for future rewards.\n","\n","    Returns:\n","    dict: A dictionary containing the learned Q-values for state-action pairs.\n","    \"\"\"\n","    q_values = defaultdict(lambda: 0)\n","    \n","    # Implementation goes here\n","            \n","    return q_values\n","\n","env = GridEnv()\n","q_values = q_learning(env)\n","optimal_policy = derive_optimal_policy(q_values)\n","display_policy_as_latex_table(optimal_policy, grid_size=5)\n","\n","value_function = compute_value_function(q_values, optimal_policy)\n","print(\"value function\", value_function)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":2}
