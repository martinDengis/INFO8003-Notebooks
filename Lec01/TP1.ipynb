{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ae4bfb",
   "metadata": {},
   "source": [
    "# TP1 INFO8003\n",
    "The idea behind this notebook is to get familiar with basic concepts of reinforcement learning such as the design of environments and the state value function $V^\\pi(s)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ac705",
   "metadata": {},
   "source": [
    "## Question 1: Implementing a Discrete Gold Miner Environment\n",
    "\n",
    "Your task is to create a discrete environment for a \"Gold Miner\" game following the [Gym API](https://gymnasium.farama.org/v0.26.3/api/env/). Specifically, you need to implement the `step` and `reset` functions:\n",
    "\n",
    "- **`step(action)`**:  \n",
    "  This method updates the environment based on the given action. It should return:  \n",
    "  1. The agent's next observation.  \n",
    "  2. The reward obtained for the action.  \n",
    "  3. Whether the episode has ended (`terminated`) or paused (`truncated`).  \n",
    "  4. Additional information, such as metrics or debugging data about the environment.  \n",
    "\n",
    "- **`reset()`**:  \n",
    "  This method initializes the environment to its starting state. It should return:  \n",
    "  1. The agent's first observation for a new episode.  \n",
    "  2. Additional information, such as metrics or debugging data.  \n",
    "\n",
    "---\n",
    "\n",
    "### Gold Miner Environment\n",
    "\n",
    "You get to visit a gold mine and you are allowed to spend as much time in it as you wish to get as much gold as possible. Working and spending time in the mine is complicated and rather than maximizing the quantity of gold mined, you want to optimize your satisfaction. You don't plan to stay in the mine for very long. Since you have followed the RL course last year, you have computed a discount factor of $95\\%$ which allows you to not spend too much time in the gold mine.\n",
    "\n",
    "You start at the entry of the mine, at the ground level. You can decide to either go one level down, stay where you are or go one level up. To get to the gold, you have to dig trough several layers of soil. Staying at ground level, does not give you any satisfaction. The first two layers cost you one point of satisfaction each. The third layer contains a little gold and rewards you with one satisfaction point. If you decide to dig deeper, the next 4 layers are more complicated. Each layer further would respectively cost you 1,2,3 and 4 satisfaction points. However, the last layer is full of gold and digging there rewards you with 10 satisfaction points. If you decide to stay on a layer, you keep digging the same layer and it provides the same satisfaction over and over again.\n",
    "\n",
    "![Gold Miner Environment](Mine.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Tuple\n",
    "\n",
    "class MinerGymEnv(Env): \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Gold Miner environment.\n",
    "        \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action in the environment and update its state.\n",
    "\n",
    "        Parameters:\n",
    "        action (int): The action taken by the agent. This could be moving up, down, or staying at the current level.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - next_state (int): The new state of the environment after the action.\n",
    "            - reward (float): The reward received for taking the action.\n",
    "            - terminated (bool): Whether the episode has ended.\n",
    "            - truncated (bool): Whether the episode was truncated (paused).\n",
    "            - info (dict): Additional information, such as metrics or debugging data.\n",
    "        \"\"\"\n",
    "        # Implementation goes here\n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "\n",
    "        This method is used to start a new episode by resetting the environment's state \n",
    "        to the initial state (ground level).\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - current_state (int): The initial state of the environment.\n",
    "            - info (dict): Additional information, such as metrics or debugging data.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.current_state, {}\n",
    "    \n",
    "    def value_function(self, agent, n):\n",
    "        \"\"\"\n",
    "        Estimate the state value function for a given policy over a specified number of iterations.\n",
    "\n",
    "        Parameters:\n",
    "        agent (Agent): The agent whose policy is used to estimate the value function.\n",
    "        n (int): The number of iterations to perform for estimating the value function.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: An array representing the estimated value of each state, indicating the expected cumulative reward for following the policy from each state.\n",
    "        \"\"\"\n",
    "        # Implementation goes here\n",
    "\n",
    "        return state_value_function_table\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c2a19",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "You are asked to code an agent that choses the always-dig policy via the function: \n",
    "- chose_action(state): that takes a state as input and returns an always-dig action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \"\"\"\n",
    "        self.action = 1\n",
    "        \n",
    "    def chose_action(self,state):\n",
    "        \"\"\"\n",
    "        Determine the action to take based on the current state.\n",
    "\n",
    "        Parameters:\n",
    "        state (int): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        int: The action chosen by the agent. In this implementation, the agent always returns\n",
    "        a predefined policy of always digging.\n",
    "        \"\"\"\n",
    "        return self.action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66941db",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "\n",
    "Interaction between agent and domain over 10 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f299407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = MinerGymEnv()\n",
    "state, info = env.reset()\n",
    "agent = Agent()\n",
    "steps = 10\n",
    "trajectory = []\n",
    "\n",
    "for _ in range(steps):\n",
    "    action = agent.chose_action(state)\n",
    "    next_state, reward, _, _, _ = env.step(action)\n",
    "    trajectory.append((state, action, reward, next_state))\n",
    "    state = next_state\n",
    "\n",
    "for i, sample in enumerate(trajectory):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0607d75",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Implement the state value function that evalues the estimated reward if a policy is applied for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.value_function(agent, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0ebbc",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "You will find here below the implementation of the grid world environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(Env):\n",
    "    def __init__(self, grid_rewards, state_0= (3, 0), deterministic=True):\n",
    "        \"\"\"\n",
    "        Initialize the GridWorld environment.\n",
    "\n",
    "        Parameters:\n",
    "        grid_rewards (list of lists): A 2D list representing the reward values for each cell in the grid.\n",
    "        state_0 (tuple): The initial state of the agent in the grid, default is (3, 0).\n",
    "        deterministic (bool): If True, the environment behaves deterministically; otherwise, it behaves stochastically.\n",
    "        \"\"\"\n",
    "        self.grid = np.array(grid_rewards)\n",
    "        self.n, self.m = self.grid.shape\n",
    "        self.action_space = Discrete(4)  # Actions: 0=Right, 1=Left, 2=Up, 3=Down\n",
    "        self.observation_space = Tuple((Discrete(self.n), Discrete(self.m)))\n",
    "        self.state = state_0\n",
    "        self.deterministic = deterministic\n",
    "        self.gamma=0.99\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action in the environment and update the agent's state.\n",
    "\n",
    "        Parameters:\n",
    "        action (int): The action to be taken by the agent. Actions are encoded as integers: \n",
    "                    0=Right, 1=Left, 2=Up, 3=Down.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - next_state (tuple): The new state of the agent after the action.\n",
    "            - reward (float): The reward received for taking the action.\n",
    "            - terminated (bool): Always False, as the environment does not have terminal states.\n",
    "            - truncated (bool): Always False, as the environment does not have truncated states.\n",
    "            - info (dict): An empty dictionary for additional information.\n",
    "        \"\"\"\n",
    "        actions = {\n",
    "            0: (0, 1),   # Right\n",
    "            1: (0, -1),  # Left\n",
    "            2: (-1, 0),  # Up\n",
    "            3: (1, 0)    # Down\n",
    "        }\n",
    "        \n",
    "        i, j = actions[action]\n",
    "        x, y = self.state\n",
    "\n",
    "        if self.deterministic:\n",
    "            next_state = (\n",
    "                min(max(x + i, 0), self.n - 1),\n",
    "                min(max(y + j, 0), self.m - 1)\n",
    "            )\n",
    "        else:\n",
    "            if np.random.uniform() <= 0.5:\n",
    "                next_state = (\n",
    "                    min(max(x + i, 0), self.n - 1),\n",
    "                    min(max(y + j, 0), self.m - 1)\n",
    "                )\n",
    "            else:\n",
    "                next_state = (0, 0)\n",
    "\n",
    "        reward = self.grid[next_state]\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, False, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        tuple: The initial state of the environment.\n",
    "        \"\"\"\n",
    "        self.state = (3, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the current state of the grid with the agent's position marked.\n",
    "\n",
    "        The grid is displayed with '.' for empty cells and 'A' for the agent's current position.\n",
    "        \"\"\"\n",
    "        grid_display = np.zeros_like(self.grid, dtype=str)\n",
    "        grid_display[:] = '.'\n",
    "        x, y = self.state\n",
    "        grid_display[x, y] = 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid_display))\n",
    "\n",
    "grid_rewards = [\n",
    "    [-3, 1, -5, 0, 19],\n",
    "    [6, 3, 8, 9, 10],\n",
    "    [5, -8, 4, 1, -8],\n",
    "    [6, -9, 4, 19, -5],\n",
    "    [-20, -17, -4, -3, 9]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63250354",
   "metadata": {},
   "source": [
    "Implement the rule-based policy \"always go right\". Simulate the policy in the domain through a single trajectory of 10 steps, starting by the initial state s0 = (3, 0). Display the trajectories as a sequence of four tuples (s0, a0, r0, s1), ... , (s10, a10, r10, s11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_policy(state):\n",
    "    \"\"\"\n",
    "    Define a rule-based policy for the agent.\n",
    "\n",
    "    Parameters:\n",
    "    state (tuple): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "    int: The action chosen by the policy.\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def simulate_policy(env, policy, steps=10):\n",
    "    \"\"\"\n",
    "    Simulate a policy in the given environment over a specified number of steps.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is to be simulated.\n",
    "    policy (function): A function that takes a state as input and returns an action.\n",
    "    steps (int): The number of steps to simulate the policy for.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples representing the trajectory, where each tuple contains:\n",
    "          - state (tuple): The state before taking the action.\n",
    "          - action (int): The action taken.\n",
    "          - reward (float): The reward received after taking the action.\n",
    "          - next_state (tuple): The state after taking the action.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "\n",
    "    # Implementation goes here\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "env = GridWorld(grid_rewards, deterministic=True)\n",
    "# Simulate trajectory\n",
    "trajectory = simulate_policy(env, rule_based_policy, steps=10)\n",
    "for t in trajectory:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1a2b3",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Implement a routine to estimate $V^μ_N$ in the deterministic domain, where $\\mu : S \\rightarrow A$ is a stationary policy. Test your implementation with your rule-based policy of Question 4. \n",
    "\n",
    "Provide a bound on the suboptimality of $\\mu^*_{100}$ with respect to $µ^*$. Is the bound a good\n",
    "one? Compute the value of N such that this bound is equal to 0.01. Motivate your choice using the discount factor $\\gamma$.\n",
    "\n",
    "Display $V^μ_N(s)$ for each state $s$.\n",
    "\n",
    "Reminder: $V^μ_N(s) = r(s,a) + \\gamma * V^μ_{N-1}(f(s, a))\\quad \\forall N \\ge 1$ with $V^μ_0(s) = 0$ in a deterministic domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b43902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_value_function(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function using a truncated approach over a specified number of iterations.\n",
    "\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using a deterministic approach to account for the environment's dynamics.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is evaluated.\n",
    "    policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "    gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "    N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array representing the estimated value of each state, indicating the expected cumulative reward for following the policy from each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "\n",
    "    # Implementation goes here\n",
    "\n",
    "    return state_value_function_table\n",
    "\n",
    "truncated_value_function_table = truncated_value_function(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44846186",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Estimate the expectation of the state value function with 10 runs using Monte-carlo estimation.\n",
    "\n",
    "Reminder: $V^μ_N(s) = \\mathbb{E}_w\\{r(s,a,w) + \\gamma * V^μ_{N-1}(f(s,a,w))\\} \\quad \\forall N \\ge 1$ with $V^μ_0(s) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67878cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_value_function_stochastic(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function using a stochastic approach over a specified number of iterations.\n",
    "\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using a Monte Carlo method to account for stochasticity in the environment.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is evaluated.\n",
    "    policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "    gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "    N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - state_value_function_table (np.ndarray): An array representing the estimated value of each state.\n",
    "        - std_state_value_function_table (np.ndarray): An array representing the standard deviation of the estimated value for each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "    std_state_value_function_table = np.zeros((env.n, env.m))\n",
    "\n",
    "    # Implementation goes here\n",
    "\n",
    "    return state_value_function_table, std_state_value_function_table\n",
    "\n",
    "truncated_value_function_table, std_truncated_value_function_table = truncated_value_function_stochastic(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)\n",
    "print(std_truncated_value_function_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ed197",
   "metadata": {},
   "source": [
    "Is your bound still valid using Monte Carlo estimation for the expectation ? Why ?\n",
    "\n",
    "## Bonus: \n",
    "How can you adjust your implementation if you know the dynamics of the environment to compute the exact $V^\\mu_N$ function ?\n",
    "Implement your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_value_function(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function in the stochastic grid world environment over a specified number of iterations.\n",
    "\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using the dynamics of the environment to account for stochasticity in the environment.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is evaluated.\n",
    "    policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "    gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "    N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - state_value_function_table (np.ndarray): An array representing the estimated value of each state.\n",
    "        - std_state_value_function_table (np.ndarray): An array representing the standard deviation of the estimated value for each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "    std_state_value_function_table = np.zeros((env.n, env.m))\n",
    "\n",
    "    return state_value_function_table, std_state_value_function_table\n",
    "\n",
    "truncated_value_function_table = truncated_value_function(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)\n",
    "print(std_truncated_value_function_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
